<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>TIP-I2V</title>
     

</head>

<body>
<div class="container">
    <div class="title">
        TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation
    </div>

    <div class="venue">
        Arxiv 2024
    </div>

    <br><br>

    <div class="author">
        <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=en">Yi Yang</a><sup>2</sup>
    </div>
    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>University of Technology Sydney</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Zhejiang University</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2411.xxxxxx">[Paper]</a></div>
    <div class="links"><a href="https://huggingface.co/datasets/WenhaoWang/TIP-I2V">[Data]</a></div>
    <div class="links"><a href="https://github.com/WangWenhao0716/TIP-I2V">[Github]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teasor.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
        TIP-I2V is the first dataset comprising over 1.70 million unique user-provided text and image prompts. Besides the prompts, TIP-I2V also includes videos generated by five state-of-the-art image-to-video models (Pika, Stable Video Diffusion, Open-Sora, I2VGen-XL, and CogVideoX-5B). The TIP-I2V contributes to the development of better and safer image-to-video models.
    </p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
         Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset.
    </p>

    <br><br>
    <hr>

    <h1>Datapoint</h1>
    <img style="width: 80%;" src="./resources/datapoint.png"
         alt="A data point in the proposed TIP-I2V"/>
    <br>
    
    <br><br>
    <hr>

    <h1>Statistics</h1>
    <img style="width: 80%;" src="./resources/stat.png"
         alt="The statistics of the proposed TIP-I2V"/>
    <br>
    
    <br><br>
    <hr>

    <h1>Examples</h1>
    <img style="width: 80%;" src="./resources/subject_all.png"
         alt="Examples of the proposed TIP-I2V"/>
    <div style="height: 20px;"></div> 
    <img style="width: 80%;" src="./resources/direct_all.png"
         alt="Examples of the proposed TIP-I2V"/>
    <br>
    <br><br>
    <hr>

    <h1>Comparison with VidProM and DiffusionDB</h1>

    <img style="width: 80%;" src="./resources/table.png" alt="Results figure"/>
    <div style="height: 20px;"></div>  
    <img style="width: 80%;" src="./resources/comparison.png" alt="Results figure"/>
    <div style="height: 20px;"></div>  

    <div style="text-align: center; margin-top: 20px;">
      <a href="https://poloclub.github.io/wizmap/?dataURL=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FWenhaoWang%2FTIP-I2V%2Fresolve%2Fmain%2Ftip-i2v-visualize%2Fdata_tip-i2v_vidprom.ndjson&gridURL=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FWenhaoWang%2FTIP-I2V%2Fresolve%2Fmain%2Ftip-i2v-visualize%2Fgrid_tip-i2v_vidprom.json">
            [Wizmap of TIP-I2V and VidProM]
      </a>
      <a href="https://poloclub.github.io/wizmap/?dataURL=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FWenhaoWang%2FTIP-I2V%2Fresolve%2Fmain%2Ftip-i2v-visualize%2Fdata_tip-i2v_diffusiondb.ndjson&gridURL=https%3A%2F%2Fhuggingface.co%2Fdatasets%2FWenhaoWang%2FTIP-I2V%2Fresolve%2Fmain%2Ftip-i2v-visualize%2Fgrid_tip-i2v_diffusiondb.json">
              [Wizmap of TIP-I2V and DiffusionDB]
      </a>
    </div>
      
    <br><br>
    <hr>



    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2411.xxxx">
            <img class="layered-paper-big" width="100%" src="./resources/TIP-I2V.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation</h3>
        <p>Wenhao Wang and Yi Yang</p>
        <p>Arxiv, 2024.</p>
        <pre><code>@article{wang2024tip-i2v,
  title={TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation},
  author={Wang, Wenhao and Yang, Yi},
  journal={arXiv preprint arXiv:2411.xxxxx},
  year={2024},
}</code></pre>
    </div>

    <br><br>
    <hr>

   <h1>Contact</h1>
    <p style="width: 80%;">
        If you have any questions, feel free to contact <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a> (wangwenhao0716@gmail.com).
    </p>

      <br><br>
    <hr>

  <h1>Works based on VidProM</h1>
    <p style="width: 80%;">
      We are actively collecting your awesome works based on our TIP-I2V. Please let us know if you finish one.
    </p>
    

   <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        We sincerely thank OpenAI for their support through the Researcher Access Program. Without their generous contribution, this work would not have been possible. 
    </p>

    <br><br>
</div>

</body>

</html>
