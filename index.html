<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>TIP-I2V</title>
     

</head>

<body>
<div class="container">
    <div class="title">
        TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation
    </div>

    <div class="venue">
        Arxiv 2024
    </div>

    <br><br>

    <div class="author">
        <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=en">Yi Yang</a><sup>2</sup>
    </div>
    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>University of Technology Sydney</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Zhejiang University</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2411.xxxxxx">[Paper]</a></div>
    <div class="links"><a href="https://huggingface.co/datasets/WenhaoWang/TIP-I2V">[Data]</a></div>
    <div class="links"><a href="https://github.com/WangWenhao0716/TIP-I2V">[Github]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teasor.png" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
        VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models.
    </p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models.
    </p>

    <br><br>
    <hr>

    <h1>Datapoint</h1>
    <img style="width: 80%;" src="./resources/datapoint.jpg"
         alt="A data point in the proposed VidProM"/>
    <br>
    
    <br><br>
    <hr>

    <h1>Basic information of VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/compare_table.jpg"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>Differences between prompts in VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/compare_visual.png"
         alt="Results figure"/>

    <br><br>
    <hr>

    <h1>WizMap visualization of prompts in VidProM and DiffusionDB</h1>
    <img style="width: 80%;" src="./resources/WizMap_V_D.jpg"
         alt="Results figure"/>
    <div class="links"><a href="https://poloclub.github.io/wizmap/?dataURL=https://huggingface.co/datasets/WenhaoWang/VidProM/resolve/main/data_vidprom_diffusiondb.ndjson&gridURL=https://huggingface.co/datasets/WenhaoWang/VidProM/resolve/main/grid_vidprom_diffusiondb.json%20">[Wizmap]</a></div>
    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2403.06098">
            <img class="layered-paper-big" width="100%" src="./resources/vidprom.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models</h3>
        <p>Wenhao Wang and Yi Yang</p>
        <p>NeurIPS, 2024.</p>
        <pre><code>@article{wang2024vidprom,
  title={VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models},
  author={Wang, Wenhao and Yang, Yi},
  journal={Thirty-eighth Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=pYNl76onJL}
}</code></pre>
    </div>

    <br><br>
    <hr>

   <h1>Contact</h1>
    <p style="width: 80%;">
        If you have any questions, feel free to contact <a href="https://wangwenhao0716.github.io/">Wenhao Wang</a> (wangwenhao0716@gmail.com).
    </p>

      <br><br>
    <hr>

  <h1>Works based on VidProM</h1>
    <p style="width: 80%;">
      We are actively collecting your awesome works based on our VidProM. Please let us know if you finish one.
    </p>
      
    <p style="width: 80%;">
    1. Ji, Lichuan, et al. "Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features." arXiv preprint arXiv:2405.15343 (2024).
    </p>
    <p style="width: 80%;">
    2. Xuan, He, et al. "VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation." arXiv preprint arXiv:2406.15252 (2024).
    </p>
    <p style="width: 80%;">
    3. Liao, Mingxiang, et al. "Evaluation of Text-to-Video Generation Models: A Dynamics Perspective." arXiv preprint arXiv:2407.01094 (2024).
    </p>
    <p style="width: 80%;">
    4. Miao, Yibo, et al. "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models." arXiv preprint arXiv:2407.05965 (2024).
    </p>
 

   <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a> and <a href="https://github.com/elliottwu/webpage-template">Shangzhe Wu</a>.
        The code can be found <a href="https://github.com/VidProM/vidprom.github.io">here</a>.
    </p>

    <br><br>
</div>

</body>

</html>
